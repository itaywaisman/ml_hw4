{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## Part a\n",
    "Under i.i.d assumptions we can show that:\n",
    "$$\n",
    "P(w | \\mu = 0, b) = \\prod_{i=1}^{m} P(w_i | \\mu=0, b) = \\\\\n",
    "\\prod_{i=1}^{m} \\frac{1}{2b}exp(-\\frac{|w_i|}{b}) = \\\\\n",
    "(2b)^{-m}exp(-\\frac{\\sum_{i=1}^{m} |w_i|}{b})\n",
    "$$\n",
    "\n",
    "\n",
    "## Part b\n",
    "\n",
    "The lasso regression problem can be written as follows:\n",
    "$$\n",
    "\\hat{w}_{LASSO}=argmin_w \\space ||Xw-y||^2 +\\lambda||w||_1\\\\\n",
    "= argmin_w \\space \\sum_{i=1}^{m}(x_i^Tw-y_i)^2+\\lambda||w||_1\n",
    "\n",
    "$$\n",
    "\n",
    "We can show that:\n",
    "$$\n",
    "p(\\{(x_i,y_i\\}_{i=1}^{m}|w,\\mu=0,b)=\\Pi_{i=1}^{m}p((x_i,y_i)|w)=\\Pi_{i=1}^{m}p(y_i|x_i,w) \\\\\n",
    "= \\Pi_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{(x_i^Tw-y_i)^2}{2}) \\\\\n",
    "= (2\\pi)^{-\\frac{m}{2}}exp(-\\frac{1}{2}\\sum_{i=1}^{m}(x_i^Tw-y_i)^2)\n",
    "$$\n",
    "Using our knowledge of the noise $\\epsilon_i\\sim\\mathcal{N}(0,1)$  \n",
    "\n",
    "We can now show that $\\hat{w}_{MAP} =\\hat{w}_{LASSO}$ using bayes theroem:\n",
    "$$\n",
    "\\hat{w}_{MAP} \\triangleq argmax_w \\space p(w | \\{(x_i, y_i)\\}_{i=1}^{m},\\mu=0,b)\\\\\n",
    "= argmax_w \\space p(\\{(x_i, y_i)\\}_{i=1}^{m} | w,\\mu=0,b)p(w |\\mu=0,b) \\\\\n",
    "= argmax_w \\space ln(p(\\{(x_i, y_i)\\}_{i=1}^{m} | w,\\mu=0,b)p(w|\\mu=0,b)) \\\\\n",
    "=  argmax_w \\space -\\frac{m}{2}\\cdot ln(2\\pi) -\\frac{1}{2}\\sum_{i=1}^{m}(x_i^Tw-y_i)^2 -m \\cdot ln(2b) -\\frac{\\sum_{i=1}^{m} |w_i|}{b} \\\\\n",
    "= argmin_w \\space \\sum_{i=1}^{m}(x_i^Tw-y_i)^2 +\\frac{1}{b}\\sum_{i=1}^{m} |w_i| \\\\\n",
    "= argmin_w \\space ||Xw-y||^2 +\\lambda||w||_1\\\\\n",
    "= \\hat{w}_{LASSO}\n",
    "$$\n",
    "\n",
    "Where the suitable parameter $\\lambda$ is $\\frac{1}{b}$.\n",
    "\n",
    "## Part c\n",
    "\n",
    "The intuition is that by looking at the figure we can see that the cdf is more \"concentrated\" around 0, than the normal-distributed case.  \n",
    "Translated to the regression (Least Squares) problem, using the above formulation, we can see that this means that most weight $w_i$ are probably close to 0, and fewer dominent weights are more distant from 0.  \n",
    "This is exactly the sparsity we are talking about, as the lasso regressor fits a weights vector $w$, in which most of the weights are very close to 0, and only a few are different from 0.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (DL_hw1)",
   "language": "python",
   "name": "pycharm-44a6c8e0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
